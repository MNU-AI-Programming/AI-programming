{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# AIí™œìš©í”„ë¡œê·¸ë˜ë° Week 9 ì‹¤ìŠµ ë…¸íŠ¸ë¶  \n## íŒŒì¼ ì…ì¶œë ¥(File I/O) + ì‹¤ìŠµ ë°ì´í„°ë¡œ ì—°ìŠµ (With AI)\n\në³¸ ë…¸íŠ¸ë¶ì€ Week 9 ê°•ì˜ìë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ, **íŒŒì¼ì„ ì½ê³ /ì“°ëŠ” ê¸°ë³¸ íë¦„**ì„ ì—°ìŠµí•˜ë„ë¡ êµ¬ì„±í–ˆìŠµë‹ˆë‹¤. îˆ€fileciteîˆ‚turn4file0îˆ\n\n---\n### ğŸ¯ ì˜¤ëŠ˜ í•™ìŠµ ëª©í‘œ\n1. í…ìŠ¤íŠ¸ íŒŒì¼ì„ ì½ëŠ” 3ê°€ì§€ ë°©ë²•(read/readline/readlines)ì„ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.  \n2. CSV/JSON íŒŒì¼ì„ ì½ê³  ê°„ë‹¨í•œ ì²˜ë¦¬(í•©ê³„, í‰ê· , ë¹ˆë„, TOP-N)ë¥¼ í•  ìˆ˜ ìˆë‹¤.  \n3. ê²½ë¡œ/ì‘ì—… í´ë”(working directory) ê°œë…ì„ ì´í•´í•œë‹¤.  \n4. LLMì„ í™œìš©í•´ â€œì½”ë“œ ìƒì„±â†’ê²€ì¦â†’ê°œì„ â€ ë£¨í‹´ì„ ì ìš©í•œë‹¤.  \n\n> ì‹¤í–‰ íŒ: ê° ì…€ì„ í´ë¦­í•˜ê³  **Shift + Enter** ë¡œ ì‹¤í–‰í•˜ì„¸ìš”.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 0. ì‹¤ìŠµ ë°ì´í„° ì¤€ë¹„ (Colab / Jupyter ê³µí†µ)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "ì´ ë…¸íŠ¸ë¶ì€ `week09_datafiles.zip` ë°ì´í„° ë¬¶ìŒì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n\n- **Colab**: ì™¼ìª½ íŒŒì¼ ì•„ì´ì½˜(ğŸ“) â†’ **ì—…ë¡œë“œ**ë¡œ zip íŒŒì¼ì„ ì˜¬ë¦° ë’¤, ì•„ë˜ ì…€ì„ ì‹¤í–‰í•˜ì„¸ìš”.  \n- **ë¡œì»¬ Jupyter**: ì´ ë…¸íŠ¸ë¶(.ipynb)ê³¼ ê°™ì€ í´ë”ì— zipì„ ë‘ê³  ì•„ë˜ ì…€ì„ ì‹¤í–‰í•˜ë©´ ë©ë‹ˆë‹¤.\n\nâœ… zip íŒŒì¼ì´ ì—†ë‹¤ë©´: ìˆ˜ì—…ì—ì„œ ì œê³µëœ `week09_datafiles.zip`ì„ ì‚¬ìš©í•˜ì„¸ìš”.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# (1) í˜„ì¬ ì‘ì—… í´ë” í™•ì¸\nimport os, pathlib\nprint(\"í˜„ì¬ ì‘ì—… í´ë”:\", os.getcwd())\nprint(\"í´ë” ë‚´ìš© ì¼ë¶€:\", os.listdir('.')[:10])\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 0-1. zip íŒŒì¼ ì••ì¶• í•´ì œ\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# (2) zip íŒŒì¼ì„ í’€ì–´ì„œ week09_datafiles/ í´ë”ë¥¼ ë§Œë“­ë‹ˆë‹¤.\nimport zipfile\n\nzip_name = \"week09_datafiles.zip\"  # í•„ìš”í•˜ë©´ íŒŒì¼ëª…ì„ ë°”ê¾¸ì„¸ìš”.\nextract_dir = \"week09_datafiles\"\n\nif not os.path.exists(zip_name):\n    print(\"âš ï¸ zip íŒŒì¼ì´ í˜„ì¬ í´ë”ì— ì—†ìŠµë‹ˆë‹¤:\", zip_name)\n    print(\"Colabì´ë¼ë©´ ì™¼ìª½ ğŸ“ì—ì„œ ì—…ë¡œë“œ í›„ ë‹¤ì‹œ ì‹¤í–‰í•˜ì„¸ìš”.\")\nelse:\n    with zipfile.ZipFile(zip_name, 'r') as z:\n        z.extractall(\".\")\n    print(\"âœ… ì••ì¶• í•´ì œ ì™„ë£Œ!\")\n    print(\"í´ë” í™•ì¸:\", os.listdir(extract_dir)[:10])\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n## 1. í…ìŠ¤íŠ¸ íŒŒì¼ ì½ê¸°: read / readline / readlines\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 1-1. read(): íŒŒì¼ ì „ì²´ë¥¼ í•œ ë²ˆì— ì½ê¸°\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "path = \"week09_datafiles/data.txt\"\n\nwith open(path, \"r\", encoding=\"utf-8\") as f:\n    content = f.read()\n\nprint(content)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 1-2. readline(): í•œ ì¤„ì”© ì½ê¸°(ì²« ì¤„ë§Œ ì˜ˆì‹œ)\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "with open(path, \"r\", encoding=\"utf-8\") as f:\n    line1 = f.readline()\n    line2 = f.readline()\n\nprint(\"1ì¤„:\", line1.strip())\nprint(\"2ì¤„:\", line2.strip())\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 1-3. readlines(): ì¤„ ë‹¨ìœ„ ë¦¬ìŠ¤íŠ¸ë¡œ ì½ê¸°\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "with open(path, \"r\", encoding=\"utf-8\") as f:\n    lines = f.readlines()\n\nprint(\"ì´ ì¤„ ìˆ˜:\", len(lines))\nprint(\"ë§ˆì§€ë§‰ ì¤„:\", lines[-1].strip())\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n## 2. ìˆ«ì íŒŒì¼ ì½ê¸°: ê³µë°±/ì¤„ë°”ê¿ˆ ì„ì¸ ë°ì´í„° ì²˜ë¦¬\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 2-1. nums.txtë¥¼ ì½ì–´ì„œ ì •ìˆ˜ ë¦¬ìŠ¤íŠ¸ë¡œ ë§Œë“¤ê¸°\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "path_nums = \"week09_datafiles/nums.txt\"\n\nwith open(path_nums, \"r\", encoding=\"utf-8\") as f:\n    raw = f.read()          # ì „ì²´ ë¬¸ìì—´\ntokens = raw.split()        # ê³µë°±(ìŠ¤í˜ì´ìŠ¤/ì¤„ë°”ê¿ˆ/íƒ­) ê¸°ì¤€ ë¶„ë¦¬\nnumbers = [int(x) for x in tokens]\n\nprint(\"ì›ë³¸ í† í° ê°œìˆ˜:\", len(tokens))\nprint(\"ìˆ«ì ë¦¬ìŠ¤íŠ¸:\", numbers)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 2-2. í•©ê³„/í‰ê· /ìµœëŒ€/ìµœì†Œ ê³„ì‚°\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "total = sum(numbers)\navg = total / len(numbers)\nprint(\"í•©ê³„:\", total)\nprint(\"í‰ê· :\", avg)\nprint(\"ìµœëŒ€:\", max(numbers))\nprint(\"ìµœì†Œ:\", min(numbers))\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 2-3. ì—£ì§€ ì¼€ì´ìŠ¤: ë¹ˆ íŒŒì¼(nums_empty.txt) ì²˜ë¦¬\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "ë¹ˆ íŒŒì¼ì„ ì½ìœ¼ë©´ ìˆ«ì ë¦¬ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.  \nì´ë•Œ í‰ê· ì„ êµ¬í•˜ë©´ **0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ì˜¤ë¥˜**ê°€ ë‚  ìˆ˜ ìˆìœ¼ë‹ˆ, ë¨¼ì € ê¸¸ì´ë¥¼ í™•ì¸í•´ì•¼ í•©ë‹ˆë‹¤.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "path_empty = \"week09_datafiles/nums_empty.txt\"\n\nwith open(path_empty, \"r\", encoding=\"utf-8\") as f:\n    raw_empty = f.read().strip()\n\ntokens_empty = raw_empty.split() if raw_empty else []\nnumbers_empty = [int(x) for x in tokens_empty]\n\nprint(\"ë¹ˆ íŒŒì¼ í† í°:\", tokens_empty)\nprint(\"ë¹ˆ íŒŒì¼ ìˆ«ì:\", numbers_empty)\n\nif len(numbers_empty) == 0:\n    print(\"âœ… ìˆ«ìê°€ ì—†ìŠµë‹ˆë‹¤. í‰ê·  ê³„ì‚°ì„ ê±´ë„ˆëœë‹ˆë‹¤.\")\nelse:\n    print(\"í‰ê· :\", sum(numbers_empty) / len(numbers_empty))\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n## 3. CSV ì½ê¸°: grade.csv ì„±ì  ë°ì´í„°\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 3-1. csv ëª¨ë“ˆë¡œ ì½ê¸°\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "import csv\n\npath_csv = \"week09_datafiles/grade.csv\"\n\nrows = []\nwith open(path_csv, \"r\", encoding=\"utf-8\") as f:\n    reader = csv.DictReader(f)\n    for row in reader:\n        rows.append(row)\n\nprint(\"í–‰ ê°œìˆ˜:\", len(rows))\nprint(\"ì²« í–‰:\", rows[0])\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 3-2. ê° í•™ìƒì˜ í‰ê·  êµ¬í•˜ê¸°\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def to_int(x):\n    return int(x)\n\nfor row in rows:\n    name = row[\"name\"]\n    korean = to_int(row[\"korean\"])\n    math = to_int(row[\"math\"])\n    english = to_int(row[\"english\"])\n    avg = (korean + math + english) / 3\n    print(f\"{name}: avg={avg:.2f}\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 3-3. í‰ê·  TOP 3 êµ¬í•˜ê¸° (ì •ë ¬)\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "avgs = []\nfor row in rows:\n    name = row[\"name\"]\n    scores = [int(row[\"korean\"]), int(row[\"math\"]), int(row[\"english\"])]\n    avgs.append((name, sum(scores)/len(scores)))\n\n# í‰ê·  ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬\navgs_sorted = sorted(avgs, key=lambda x: x[1], reverse=True)\n\nprint(\"TOP 3\")\nfor name, a in avgs_sorted[:3]:\n    print(f\"- {name}: {a:.2f}\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 3-4. ê²°ì¸¡ì¹˜ê°€ ìˆëŠ” CSV(grade_missing.csv) ë‹¤ë£¨ê¸°(ì‹¬í™”)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "`grade_missing.csv`ì—ëŠ” ë¹ˆ ì¹¸(ê²°ì¸¡)ì´ ìˆìŠµë‹ˆë‹¤.  \nì˜¤ëŠ˜ì€ ê°„ë‹¨íˆ **ë¹ˆ ì¹¸ì€ 0ì ìœ¼ë¡œ ì²˜ë¦¬**í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì—°ìŠµí•´ë´…ë‹ˆë‹¤.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "path_missing = \"week09_datafiles/grade_missing.csv\"\n\ndef safe_int(x, default=0):\n    x = (x or \"\").strip()\n    return int(x) if x != \"\" else default\n\nrows2 = []\nwith open(path_missing, \"r\", encoding=\"utf-8\") as f:\n    reader = csv.DictReader(f)\n    for row in reader:\n        rows2.append(row)\n\nfor row in rows2:\n    name = row[\"name\"]\n    scores = [safe_int(row[\"korean\"]), safe_int(row[\"math\"]), safe_int(row[\"english\"])]\n    avg = sum(scores)/len(scores)\n    print(f\"{name}: scores={scores} avg={avg:.2f}\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n## 4. JSON ì½ê¸°: settings.json / contacts_seed.json\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 4-1. settings.json ì½ê¸°\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "import json\n\npath_settings = \"week09_datafiles/settings.json\"\nwith open(path_settings, \"r\", encoding=\"utf-8\") as f:\n    settings = json.load(f)\n\nprint(settings)\nprint(\"sound:\", settings[\"sound\"])\nprint(\"level:\", settings[\"level\"])\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 4-2. contacts_seed.jsonì„ dictë¡œ ì½ê³  ê²€ìƒ‰í•˜ê¸°\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "path_contacts = \"week09_datafiles/contacts_seed.json\"\nwith open(path_contacts, \"r\", encoding=\"utf-8\") as f:\n    contacts = json.load(f)\n\nprint(\"ì´ˆê¸° ì—°ë½ì²˜:\", contacts)\n\nname = \"Kim\"\nprint(f\"{name} ì „í™”ë²ˆí˜¸:\", contacts.get(name, \"(ì—†ìŒ)\"))\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 4-3. ì—°ë½ì²˜ ì¶”ê°€/ìˆ˜ì • í›„ ë‹¤ì‹œ JSONìœ¼ë¡œ ì €ì¥í•˜ê¸°\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "contacts[\"Choi\"] = \"010-0000-0000\"  # ì¶”ê°€(ë˜ëŠ” ìˆ˜ì •)\nout_path = \"week09_datafiles/contacts_updated.json\"\n\nwith open(out_path, \"w\", encoding=\"utf-8\") as f:\n    json.dump(contacts, f, ensure_ascii=False, indent=2)\n\nprint(\"âœ… ì €ì¥ ì™„ë£Œ:\", out_path)\nprint(\"ì €ì¥ëœ ë‚´ìš© ì¼ë¶€:\")\nwith open(out_path, \"r\", encoding=\"utf-8\") as f:\n    print(f.read())\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n## 5. í…ìŠ¤íŠ¸ ë¶„ì„ ì‹¤ìŠµ: ë‹¨ì–´ ë¹ˆë„ TOP-N\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 5-1. sentence.txt ì½ê³  ë‹¨ì–´ ë¹ˆë„ìˆ˜ ì„¸ê¸°\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "path_sentence = \"week09_datafiles/sentence.txt\"\n\nwith open(path_sentence, \"r\", encoding=\"utf-8\") as f:\n    text = f.read().lower()\n\nwords = text.split()\nfreq = {}\nfor w in words:\n    freq[w] = freq.get(w, 0) + 1\n\nprint(\"ë¹ˆë„:\", freq)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 5-2. TOP 3 ì¶œë ¥\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "top3 = sorted(freq.items(), key=lambda x: x[1], reverse=True)[:3]\nfor w, c in top3:\n    print(f\"{w}: {c}\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n## 6. set ì‹¤ìŠµ: ë¡œë˜ ë‹¹ì²¨ ë²ˆí˜¸ì™€ êµì§‘í•©\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 6-1. lotto_winning.json ì½ê¸°\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "path_lotto = \"week09_datafiles/lotto_winning.json\"\nwith open(path_lotto, \"r\", encoding=\"utf-8\") as f:\n    lotto = json.load(f)\n\nwinning = set(lotto[\"winning_numbers\"])\nprint(\"ë‹¹ì²¨ ë²ˆí˜¸:\", winning)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 6-2. ë‚´ ë²ˆí˜¸(ì˜ˆì‹œ)ì™€ êµì§‘í•©\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "my_numbers = {3, 7, 11, 20, 35, 45}\nmatched = my_numbers & winning\n\nprint(\"ë‚´ ë²ˆí˜¸:\", my_numbers)\nprint(\"ë§ì¶˜ ë²ˆí˜¸:\", matched)\nprint(\"ë§ì¶˜ ê°œìˆ˜:\", len(matched))\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n## 7. íŒŒì¼ ì“°ê¸° ì‹¤ìŠµ: ê²°ê³¼ë¥¼ report.txtë¡œ ì €ì¥\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "report_path = \"week09_datafiles/report.txt\"\n\nlines = []\nlines.append(\"Week 9 ì‹¤ìŠµ ë¦¬í¬íŠ¸\")\nlines.append(\"=\"*20)\nlines.append(f\"nums.txt í‰ê· : {sum(numbers)/len(numbers):.2f}\")\nlines.append(f\"grade.csv TOP1: {avgs_sorted[0][0]} ({avgs_sorted[0][1]:.2f})\")\nlines.append(f\"word TOP1: {top3[0][0]} ({top3[0][1]})\")\nlines.append(f\"lotto match count: {len(matched)}\")\n\nwith open(report_path, \"w\", encoding=\"utf-8\") as f:\n    f.write(\"\\n\".join(lines))\n\nprint(\"âœ… ì €ì¥ ì™„ë£Œ:\", report_path)\nprint(\"--- íŒŒì¼ ë‚´ìš© ---\")\nwith open(report_path, \"r\", encoding=\"utf-8\") as f:\n    print(f.read())\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n## 8. With AI í”„ë¡¬í”„íŠ¸(ì¶”ì²œ)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 8-1. íŒŒì¼/ê²½ë¡œ ì˜¤ë¥˜ ì§ˆë¬¸ í…œí”Œë¦¿\nì•„ë˜ í˜•ì‹ìœ¼ë¡œ AIì—ê²Œ ì§ˆë¬¸í•˜ë©´ ì •í™•ë„ê°€ ì˜¬ë¼ê°‘ë‹ˆë‹¤.\n\n```text\nì—­í• : ë„ˆëŠ” íŒŒì´ì¬ ë””ë²„ê¹… ì½”ì¹˜ì•¼.\nëª©í‘œ: ì•„ë˜ ì˜¤ë¥˜ë¥¼ í•´ê²°í•˜ëŠ” ì›ì¸ í›„ë³´ 3ê°œì™€ ë‹¨ê³„ë³„ í•´ê²° ì ˆì°¨ë¥¼ ì•Œë ¤ì¤˜.\në‚´ í™˜ê²½:\n- OS:\n- ì‹¤í–‰í•œ ì½”ë“œ/ëª…ë ¹ì–´:\n- íŒŒì¼ ìœ„ì¹˜(í´ë” êµ¬ì¡°):\nì˜¤ë¥˜ ë©”ì‹œì§€(ê·¸ëŒ€ë¡œ):\n```\n```\n\n### 8-2. ì½”ë“œ ê°œì„  ìš”ì²­ í…œí”Œë¦¿\n```text\nì—­í• : ë„ˆëŠ” íŒŒì´ì¬ íŠœí„°ì•¼.\nëª©í‘œ: ì•„ë˜ ì½”ë“œë¥¼ ë” ì•ˆì „í•˜ê²Œ(ì—£ì§€ ì¼€ì´ìŠ¤ í¬í•¨) ë¦¬íŒ©í„°ë§í•´ì¤˜.\nì œì•½: ì´ˆë³´ê°€ ì´í•´í•  ìˆ˜ ìˆê²Œ, ì£¼ì„ì€ í•µì‹¬ë§Œ, í…ŒìŠ¤íŠ¸ 3ê°œ í¬í•¨.\nì½”ë“œ:\n```python\n# ì—¬ê¸°ì— ë‚´ ì½”ë“œ\n```\n```\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n## 9. ë¯¸ë‹ˆ í€´ì¦ˆ(ê°œë… í™•ì¸)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "1) `open(..., encoding=\"utf-8\")`ë¥¼ ì“°ëŠ” ì´ìœ ëŠ”?  \n2) `read()` vs `readlines()` ì°¨ì´ í•œ ê°€ì§€ëŠ”?  \n3) CSVì—ì„œ í—¤ë”(ì»¬ëŸ¼ëª…)ë¥¼ ì´ìš©í•´ ì½ì„ ë•Œ ì“°ëŠ” ë„êµ¬ëŠ”? (`csv.???`)  \n4) JSON íŒŒì¼ì„ íŒŒì´ì¬ dictë¡œ ì½ëŠ” í•¨ìˆ˜ëŠ”?  \n5) íŒŒì¼ì´ ë¹„ì–´ìˆì„ ë•Œ í‰ê·  ê³„ì‚°ì—ì„œ ì¡°ì‹¬í•  ì ì€?\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "âœ… ì—¬ê¸°ì— ë‹µì„ ì ìœ¼ì„¸ìš”:\n\n- 1)\n- 2)\n- 3)\n- 4)\n- 5)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n## 10. ê³¼ì œ(ì œì¶œìš©)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### ê¸°ë³¸ ê³¼ì œ\n1) `nums.txt`ë¥¼ ì½ì–´ **í•©/í‰ê· /ìµœëŒ€/ìµœì†Œ**ë¥¼ ì¶œë ¥í•˜ê³ , ê²°ê³¼ë¥¼ `result_nums.txt`ë¡œ ì €ì¥  \n2) `sentence.txt` ë‹¨ì–´ ë¹ˆë„ TOP 5ë¥¼ ì¶œë ¥í•˜ê³ , ê²°ê³¼ë¥¼ `result_words.txt`ë¡œ ì €ì¥  \n3) ì‹¤í–‰ ìº¡ì²˜ 1ì¥(í„°ë¯¸ë„/ë…¸íŠ¸ë¶ ì¶œë ¥ì´ ë³´ì´ê²Œ)\n\n### ë„ì „ ê³¼ì œ\n- `grade_missing.csv` ê²°ì¸¡ ì²˜ë¦¬ ê·œì¹™ì„ **ì§ì ‘ ì •í•´ì„œ** í‰ê·  TOP 3 ì¶œë ¥\n  - ì˜ˆ: ê²°ì¸¡ì€ ì œì™¸í•˜ê³  í‰ê·  ê³„ì‚° / ê²°ì¸¡ì€ 0ìœ¼ë¡œ ì²˜ë¦¬ ë“±\n- ê·œì¹™ì„ READMEì— 3ì¤„ë¡œ ì„¤ëª…\n\nâœ… ì œì¶œë¬¼: `.py` 1ê°œ(ë˜ëŠ” `.ipynb`) + ê²°ê³¼ txt 2ê°œ + ìº¡ì²˜ 1ì¥\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    },
    "colab": {
      "name": "AI_Programming_with_AI_Week09.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}